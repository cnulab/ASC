import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import math

_weights_dict = dict()

def load_weights(weight_file):
    if weight_file == None:
        return

    try:
        weights_dict = np.load(weight_file, allow_pickle=True).item()
    except:
        weights_dict = np.load(weight_file, allow_pickle=True, encoding='bytes').item()

    return weights_dict

class ResNet50(nn.Module):

    def __init__(self, weight_file=None):
        super(ResNet50, self).__init__()
        global _weights_dict
        _weights_dict = load_weights(weight_file)

        self.conv1_7x7_s2 = self.__conv(2, name='conv1/7x7_s2', in_channels=3, out_channels=64, kernel_size=(7, 7), stride=(2, 2), groups=1, bias=False)
        self.conv1_7x7_s2_bn = self.__batch_normalization(2, 'conv1/7x7_s2/bn', num_features=64, eps=0.0010000000474974513, momentum=0.0)
        self.conv2_1_1x1_reduce = self.__conv(2, name='conv2_1_1x1_reduce', in_channels=64, out_channels=64, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.conv2_1_1x1_proj = self.__conv(2, name='conv2_1_1x1_proj', in_channels=64, out_channels=256, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.conv2_1_1x1_reduce_bn = self.__batch_normalization(2, 'conv2_1_1x1_reduce/bn', num_features=64, eps=0.0010000000474974513, momentum=0.0)
        self.conv2_1_1x1_proj_bn = self.__batch_normalization(2, 'conv2_1_1x1_proj/bn', num_features=256, eps=0.0010000000474974513, momentum=0.0)
        self.conv2_1_3x3 = self.__conv(2, name='conv2_1_3x3', in_channels=64, out_channels=64, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)
        self.conv2_1_3x3_bn = self.__batch_normalization(2, 'conv2_1_3x3/bn', num_features=64, eps=0.0010000000474974513, momentum=0.0)
        self.conv2_1_1x1_increase = self.__conv(2, name='conv2_1_1x1_increase', in_channels=64, out_channels=256, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.conv2_1_1x1_increase_bn = self.__batch_normalization(2, 'conv2_1_1x1_increase/bn', num_features=256, eps=0.0010000000474974513, momentum=0.0)
        self.conv2_2_1x1_reduce = self.__conv(2, name='conv2_2_1x1_reduce', in_channels=256, out_channels=64, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.conv2_2_1x1_reduce_bn = self.__batch_normalization(2, 'conv2_2_1x1_reduce/bn', num_features=64, eps=0.0010000000474974513, momentum=0.0)
        self.conv2_2_3x3 = self.__conv(2, name='conv2_2_3x3', in_channels=64, out_channels=64, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)
        self.conv2_2_3x3_bn = self.__batch_normalization(2, 'conv2_2_3x3/bn', num_features=64, eps=0.0010000000474974513, momentum=0.0)
        self.conv2_2_1x1_increase = self.__conv(2, name='conv2_2_1x1_increase', in_channels=64, out_channels=256, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.conv2_2_1x1_increase_bn = self.__batch_normalization(2, 'conv2_2_1x1_increase/bn', num_features=256, eps=0.0010000000474974513, momentum=0.0)
        self.conv2_3_1x1_reduce = self.__conv(2, name='conv2_3_1x1_reduce', in_channels=256, out_channels=64, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.conv2_3_1x1_reduce_bn = self.__batch_normalization(2, 'conv2_3_1x1_reduce/bn', num_features=64, eps=0.0010000000474974513, momentum=0.0)
        self.conv2_3_3x3 = self.__conv(2, name='conv2_3_3x3', in_channels=64, out_channels=64, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)
        self.conv2_3_3x3_bn = self.__batch_normalization(2, 'conv2_3_3x3/bn', num_features=64, eps=0.0010000000474974513, momentum=0.0)
        self.conv2_3_1x1_increase = self.__conv(2, name='conv2_3_1x1_increase', in_channels=64, out_channels=256, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.conv2_3_1x1_increase_bn = self.__batch_normalization(2, 'conv2_3_1x1_increase/bn', num_features=256, eps=0.0010000000474974513, momentum=0.0)
        self.conv3_1_1x1_reduce = self.__conv(2, name='conv3_1_1x1_reduce', in_channels=256, out_channels=128, kernel_size=(1, 1), stride=(2, 2), groups=1, bias=False)
        self.conv3_1_1x1_proj = self.__conv(2, name='conv3_1_1x1_proj', in_channels=256, out_channels=512, kernel_size=(1, 1), stride=(2, 2), groups=1, bias=False)
        self.conv3_1_1x1_reduce_bn = self.__batch_normalization(2, 'conv3_1_1x1_reduce/bn', num_features=128, eps=0.0010000000474974513, momentum=0.0)
        self.conv3_1_1x1_proj_bn = self.__batch_normalization(2, 'conv3_1_1x1_proj/bn', num_features=512, eps=0.0010000000474974513, momentum=0.0)
        self.conv3_1_3x3 = self.__conv(2, name='conv3_1_3x3', in_channels=128, out_channels=128, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)
        self.conv3_1_3x3_bn = self.__batch_normalization(2, 'conv3_1_3x3/bn', num_features=128, eps=0.0010000000474974513, momentum=0.0)
        self.conv3_1_1x1_increase = self.__conv(2, name='conv3_1_1x1_increase', in_channels=128, out_channels=512, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.conv3_1_1x1_increase_bn = self.__batch_normalization(2, 'conv3_1_1x1_increase/bn', num_features=512, eps=0.0010000000474974513, momentum=0.0)
        self.conv3_2_1x1_reduce = self.__conv(2, name='conv3_2_1x1_reduce', in_channels=512, out_channels=128, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.conv3_2_1x1_reduce_bn = self.__batch_normalization(2, 'conv3_2_1x1_reduce/bn', num_features=128, eps=0.0010000000474974513, momentum=0.0)
        self.conv3_2_3x3 = self.__conv(2, name='conv3_2_3x3', in_channels=128, out_channels=128, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)
        self.conv3_2_3x3_bn = self.__batch_normalization(2, 'conv3_2_3x3/bn', num_features=128, eps=0.0010000000474974513, momentum=0.0)
        self.conv3_2_1x1_increase = self.__conv(2, name='conv3_2_1x1_increase', in_channels=128, out_channels=512, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.conv3_2_1x1_increase_bn = self.__batch_normalization(2, 'conv3_2_1x1_increase/bn', num_features=512, eps=0.0010000000474974513, momentum=0.0)
        self.conv3_3_1x1_reduce = self.__conv(2, name='conv3_3_1x1_reduce', in_channels=512, out_channels=128, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.conv3_3_1x1_reduce_bn = self.__batch_normalization(2, 'conv3_3_1x1_reduce/bn', num_features=128, eps=0.0010000000474974513, momentum=0.0)
        self.conv3_3_3x3 = self.__conv(2, name='conv3_3_3x3', in_channels=128, out_channels=128, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)
        self.conv3_3_3x3_bn = self.__batch_normalization(2, 'conv3_3_3x3/bn', num_features=128, eps=0.0010000000474974513, momentum=0.0)
        self.conv3_3_1x1_increase = self.__conv(2, name='conv3_3_1x1_increase', in_channels=128, out_channels=512, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.conv3_3_1x1_increase_bn = self.__batch_normalization(2, 'conv3_3_1x1_increase/bn', num_features=512, eps=0.0010000000474974513, momentum=0.0)
        self.conv3_4_1x1_reduce = self.__conv(2, name='conv3_4_1x1_reduce', in_channels=512, out_channels=128, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.conv3_4_1x1_reduce_bn = self.__batch_normalization(2, 'conv3_4_1x1_reduce/bn', num_features=128, eps=0.0010000000474974513, momentum=0.0)
        self.conv3_4_3x3 = self.__conv(2, name='conv3_4_3x3', in_channels=128, out_channels=128, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)
        self.conv3_4_3x3_bn = self.__batch_normalization(2, 'conv3_4_3x3/bn', num_features=128, eps=0.0010000000474974513, momentum=0.0)
        self.conv3_4_1x1_increase = self.__conv(2, name='conv3_4_1x1_increase', in_channels=128, out_channels=512, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.conv3_4_1x1_increase_bn = self.__batch_normalization(2, 'conv3_4_1x1_increase/bn', num_features=512, eps=0.0010000000474974513, momentum=0.0)
        self.conv4_1_1x1_reduce = self.__conv(2, name='conv4_1_1x1_reduce', in_channels=512, out_channels=256, kernel_size=(1, 1), stride=(2, 2), groups=1, bias=False)
        self.conv4_1_1x1_proj = self.__conv(2, name='conv4_1_1x1_proj', in_channels=512, out_channels=1024, kernel_size=(1, 1), stride=(2, 2), groups=1, bias=False)
        self.conv4_1_1x1_reduce_bn = self.__batch_normalization(2, 'conv4_1_1x1_reduce/bn', num_features=256, eps=0.0010000000474974513, momentum=0.0)
        self.conv4_1_1x1_proj_bn = self.__batch_normalization(2, 'conv4_1_1x1_proj/bn', num_features=1024, eps=0.0010000000474974513, momentum=0.0)
        self.conv4_1_3x3 = self.__conv(2, name='conv4_1_3x3', in_channels=256, out_channels=256, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)
        self.conv4_1_3x3_bn = self.__batch_normalization(2, 'conv4_1_3x3/bn', num_features=256, eps=0.0010000000474974513, momentum=0.0)
        self.conv4_1_1x1_increase = self.__conv(2, name='conv4_1_1x1_increase', in_channels=256, out_channels=1024, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.conv4_1_1x1_increase_bn = self.__batch_normalization(2, 'conv4_1_1x1_increase/bn', num_features=1024, eps=0.0010000000474974513, momentum=0.0)
        self.conv4_2_1x1_reduce = self.__conv(2, name='conv4_2_1x1_reduce', in_channels=1024, out_channels=256, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.conv4_2_1x1_reduce_bn = self.__batch_normalization(2, 'conv4_2_1x1_reduce/bn', num_features=256, eps=0.0010000000474974513, momentum=0.0)
        self.conv4_2_3x3 = self.__conv(2, name='conv4_2_3x3', in_channels=256, out_channels=256, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)
        self.conv4_2_3x3_bn = self.__batch_normalization(2, 'conv4_2_3x3/bn', num_features=256, eps=0.0010000000474974513, momentum=0.0)
        self.conv4_2_1x1_increase = self.__conv(2, name='conv4_2_1x1_increase', in_channels=256, out_channels=1024, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.conv4_2_1x1_increase_bn = self.__batch_normalization(2, 'conv4_2_1x1_increase/bn', num_features=1024, eps=0.0010000000474974513, momentum=0.0)
        self.conv4_3_1x1_reduce = self.__conv(2, name='conv4_3_1x1_reduce', in_channels=1024, out_channels=256, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.conv4_3_1x1_reduce_bn = self.__batch_normalization(2, 'conv4_3_1x1_reduce/bn', num_features=256, eps=0.0010000000474974513, momentum=0.0)
        self.conv4_3_3x3 = self.__conv(2, name='conv4_3_3x3', in_channels=256, out_channels=256, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)
        self.conv4_3_3x3_bn = self.__batch_normalization(2, 'conv4_3_3x3/bn', num_features=256, eps=0.0010000000474974513, momentum=0.0)
        self.conv4_3_1x1_increase = self.__conv(2, name='conv4_3_1x1_increase', in_channels=256, out_channels=1024, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.conv4_3_1x1_increase_bn = self.__batch_normalization(2, 'conv4_3_1x1_increase/bn', num_features=1024, eps=0.0010000000474974513, momentum=0.0)
        self.conv4_4_1x1_reduce = self.__conv(2, name='conv4_4_1x1_reduce', in_channels=1024, out_channels=256, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.conv4_4_1x1_reduce_bn = self.__batch_normalization(2, 'conv4_4_1x1_reduce/bn', num_features=256, eps=0.0010000000474974513, momentum=0.0)
        self.conv4_4_3x3 = self.__conv(2, name='conv4_4_3x3', in_channels=256, out_channels=256, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)
        self.conv4_4_3x3_bn = self.__batch_normalization(2, 'conv4_4_3x3/bn', num_features=256, eps=0.0010000000474974513, momentum=0.0)
        self.conv4_4_1x1_increase = self.__conv(2, name='conv4_4_1x1_increase', in_channels=256, out_channels=1024, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.conv4_4_1x1_increase_bn = self.__batch_normalization(2, 'conv4_4_1x1_increase/bn', num_features=1024, eps=0.0010000000474974513, momentum=0.0)
        self.conv4_5_1x1_reduce = self.__conv(2, name='conv4_5_1x1_reduce', in_channels=1024, out_channels=256, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.conv4_5_1x1_reduce_bn = self.__batch_normalization(2, 'conv4_5_1x1_reduce/bn', num_features=256, eps=0.0010000000474974513, momentum=0.0)
        self.conv4_5_3x3 = self.__conv(2, name='conv4_5_3x3', in_channels=256, out_channels=256, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)
        self.conv4_5_3x3_bn = self.__batch_normalization(2, 'conv4_5_3x3/bn', num_features=256, eps=0.0010000000474974513, momentum=0.0)
        self.conv4_5_1x1_increase = self.__conv(2, name='conv4_5_1x1_increase', in_channels=256, out_channels=1024, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.conv4_5_1x1_increase_bn = self.__batch_normalization(2, 'conv4_5_1x1_increase/bn', num_features=1024, eps=0.0010000000474974513, momentum=0.0)
        self.conv4_6_1x1_reduce = self.__conv(2, name='conv4_6_1x1_reduce', in_channels=1024, out_channels=256, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.conv4_6_1x1_reduce_bn = self.__batch_normalization(2, 'conv4_6_1x1_reduce/bn', num_features=256, eps=0.0010000000474974513, momentum=0.0)
        self.conv4_6_3x3 = self.__conv(2, name='conv4_6_3x3', in_channels=256, out_channels=256, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)
        self.conv4_6_3x3_bn = self.__batch_normalization(2, 'conv4_6_3x3/bn', num_features=256, eps=0.0010000000474974513, momentum=0.0)
        self.conv4_6_1x1_increase = self.__conv(2, name='conv4_6_1x1_increase', in_channels=256, out_channels=1024, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.conv4_6_1x1_increase_bn = self.__batch_normalization(2, 'conv4_6_1x1_increase/bn', num_features=1024, eps=0.0010000000474974513, momentum=0.0)
        self.conv5_1_1x1_reduce = self.__conv(2, name='conv5_1_1x1_reduce', in_channels=1024, out_channels=512, kernel_size=(1, 1), stride=(2, 2), groups=1, bias=False)
        self.conv5_1_1x1_proj = self.__conv(2, name='conv5_1_1x1_proj', in_channels=1024, out_channels=2048, kernel_size=(1, 1), stride=(2, 2), groups=1, bias=False)
        self.conv5_1_1x1_reduce_bn = self.__batch_normalization(2, 'conv5_1_1x1_reduce/bn', num_features=512, eps=0.0010000000474974513, momentum=0.0)
        self.conv5_1_1x1_proj_bn = self.__batch_normalization(2, 'conv5_1_1x1_proj/bn', num_features=2048, eps=0.0010000000474974513, momentum=0.0)
        self.conv5_1_3x3 = self.__conv(2, name='conv5_1_3x3', in_channels=512, out_channels=512, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)
        self.conv5_1_3x3_bn = self.__batch_normalization(2, 'conv5_1_3x3/bn', num_features=512, eps=0.0010000000474974513, momentum=0.0)
        self.conv5_1_1x1_increase = self.__conv(2, name='conv5_1_1x1_increase', in_channels=512, out_channels=2048, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.conv5_1_1x1_increase_bn = self.__batch_normalization(2, 'conv5_1_1x1_increase/bn', num_features=2048, eps=0.0010000000474974513, momentum=0.0)
        self.conv5_2_1x1_reduce = self.__conv(2, name='conv5_2_1x1_reduce', in_channels=2048, out_channels=512, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.conv5_2_1x1_reduce_bn = self.__batch_normalization(2, 'conv5_2_1x1_reduce/bn', num_features=512, eps=0.0010000000474974513, momentum=0.0)
        self.conv5_2_3x3 = self.__conv(2, name='conv5_2_3x3', in_channels=512, out_channels=512, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)
        self.conv5_2_3x3_bn = self.__batch_normalization(2, 'conv5_2_3x3/bn', num_features=512, eps=0.0010000000474974513, momentum=0.0)
        self.conv5_2_1x1_increase = self.__conv(2, name='conv5_2_1x1_increase', in_channels=512, out_channels=2048, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.conv5_2_1x1_increase_bn = self.__batch_normalization(2, 'conv5_2_1x1_increase/bn', num_features=2048, eps=0.0010000000474974513, momentum=0.0)
        self.conv5_3_1x1_reduce = self.__conv(2, name='conv5_3_1x1_reduce', in_channels=2048, out_channels=512, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.conv5_3_1x1_reduce_bn = self.__batch_normalization(2, 'conv5_3_1x1_reduce/bn', num_features=512, eps=0.0010000000474974513, momentum=0.0)
        self.conv5_3_3x3 = self.__conv(2, name='conv5_3_3x3', in_channels=512, out_channels=512, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=False)
        self.conv5_3_3x3_bn = self.__batch_normalization(2, 'conv5_3_3x3/bn', num_features=512, eps=0.0010000000474974513, momentum=0.0)
        self.conv5_3_1x1_increase = self.__conv(2, name='conv5_3_1x1_increase', in_channels=512, out_channels=2048, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)
        self.conv5_3_1x1_increase_bn = self.__batch_normalization(2, 'conv5_3_1x1_increase/bn', num_features=2048, eps=0.0010000000474974513, momentum=0.0)

    def forward(self, x):
        conv1_7x7_s2_pad = F.pad(x, (2, 3, 2, 3))
        conv1_7x7_s2    = self.conv1_7x7_s2(conv1_7x7_s2_pad)
        conv1_7x7_s2_bn = self.conv1_7x7_s2_bn(conv1_7x7_s2)
        activation_1    = F.relu(conv1_7x7_s2_bn)
        max_pooling2d_1, max_pooling2d_1_idx = F.max_pool2d(activation_1, kernel_size=(3, 3), stride=(2, 2), padding=0, ceil_mode=False, return_indices=True)
        conv2_1_1x1_reduce = self.conv2_1_1x1_reduce(max_pooling2d_1)
        conv2_1_1x1_proj = self.conv2_1_1x1_proj(max_pooling2d_1)
        conv2_1_1x1_reduce_bn = self.conv2_1_1x1_reduce_bn(conv2_1_1x1_reduce)
        conv2_1_1x1_proj_bn = self.conv2_1_1x1_proj_bn(conv2_1_1x1_proj)
        activation_2    = F.relu(conv2_1_1x1_reduce_bn)
        conv2_1_3x3_pad = F.pad(activation_2, (1, 1, 1, 1))
        conv2_1_3x3     = self.conv2_1_3x3(conv2_1_3x3_pad)
        conv2_1_3x3_bn  = self.conv2_1_3x3_bn(conv2_1_3x3)
        activation_3    = F.relu(conv2_1_3x3_bn)
        conv2_1_1x1_increase = self.conv2_1_1x1_increase(activation_3)
        conv2_1_1x1_increase_bn = self.conv2_1_1x1_increase_bn(conv2_1_1x1_increase)
        add_1           = conv2_1_1x1_increase_bn + conv2_1_1x1_proj_bn
        activation_4    = F.relu(add_1)
        conv2_2_1x1_reduce = self.conv2_2_1x1_reduce(activation_4)
        conv2_2_1x1_reduce_bn = self.conv2_2_1x1_reduce_bn(conv2_2_1x1_reduce)
        activation_5    = F.relu(conv2_2_1x1_reduce_bn)
        conv2_2_3x3_pad = F.pad(activation_5, (1, 1, 1, 1))
        conv2_2_3x3     = self.conv2_2_3x3(conv2_2_3x3_pad)
        conv2_2_3x3_bn  = self.conv2_2_3x3_bn(conv2_2_3x3)
        activation_6    = F.relu(conv2_2_3x3_bn)
        conv2_2_1x1_increase = self.conv2_2_1x1_increase(activation_6)
        conv2_2_1x1_increase_bn = self.conv2_2_1x1_increase_bn(conv2_2_1x1_increase)
        add_2           = conv2_2_1x1_increase_bn + activation_4
        activation_7    = F.relu(add_2)
        conv2_3_1x1_reduce = self.conv2_3_1x1_reduce(activation_7)
        conv2_3_1x1_reduce_bn = self.conv2_3_1x1_reduce_bn(conv2_3_1x1_reduce)
        activation_8    = F.relu(conv2_3_1x1_reduce_bn)
        conv2_3_3x3_pad = F.pad(activation_8, (1, 1, 1, 1))
        conv2_3_3x3     = self.conv2_3_3x3(conv2_3_3x3_pad)
        conv2_3_3x3_bn  = self.conv2_3_3x3_bn(conv2_3_3x3)
        activation_9    = F.relu(conv2_3_3x3_bn)
        conv2_3_1x1_increase = self.conv2_3_1x1_increase(activation_9)
        conv2_3_1x1_increase_bn = self.conv2_3_1x1_increase_bn(conv2_3_1x1_increase)
        add_3           = conv2_3_1x1_increase_bn + activation_7
        activation_10   = F.relu(add_3)
        conv3_1_1x1_reduce = self.conv3_1_1x1_reduce(activation_10)
        conv3_1_1x1_proj = self.conv3_1_1x1_proj(activation_10)
        conv3_1_1x1_reduce_bn = self.conv3_1_1x1_reduce_bn(conv3_1_1x1_reduce)
        conv3_1_1x1_proj_bn = self.conv3_1_1x1_proj_bn(conv3_1_1x1_proj)
        activation_11   = F.relu(conv3_1_1x1_reduce_bn)
        conv3_1_3x3_pad = F.pad(activation_11, (1, 1, 1, 1))
        conv3_1_3x3     = self.conv3_1_3x3(conv3_1_3x3_pad)
        conv3_1_3x3_bn  = self.conv3_1_3x3_bn(conv3_1_3x3)
        activation_12   = F.relu(conv3_1_3x3_bn)
        conv3_1_1x1_increase = self.conv3_1_1x1_increase(activation_12)
        conv3_1_1x1_increase_bn = self.conv3_1_1x1_increase_bn(conv3_1_1x1_increase)
        add_4           = conv3_1_1x1_increase_bn + conv3_1_1x1_proj_bn
        activation_13   = F.relu(add_4)
        conv3_2_1x1_reduce = self.conv3_2_1x1_reduce(activation_13)
        conv3_2_1x1_reduce_bn = self.conv3_2_1x1_reduce_bn(conv3_2_1x1_reduce)
        activation_14   = F.relu(conv3_2_1x1_reduce_bn)
        conv3_2_3x3_pad = F.pad(activation_14, (1, 1, 1, 1))
        conv3_2_3x3     = self.conv3_2_3x3(conv3_2_3x3_pad)
        conv3_2_3x3_bn  = self.conv3_2_3x3_bn(conv3_2_3x3)
        activation_15   = F.relu(conv3_2_3x3_bn)
        conv3_2_1x1_increase = self.conv3_2_1x1_increase(activation_15)
        conv3_2_1x1_increase_bn = self.conv3_2_1x1_increase_bn(conv3_2_1x1_increase)
        add_5           = conv3_2_1x1_increase_bn + activation_13
        activation_16   = F.relu(add_5)
        conv3_3_1x1_reduce = self.conv3_3_1x1_reduce(activation_16)
        conv3_3_1x1_reduce_bn = self.conv3_3_1x1_reduce_bn(conv3_3_1x1_reduce)
        activation_17   = F.relu(conv3_3_1x1_reduce_bn)
        conv3_3_3x3_pad = F.pad(activation_17, (1, 1, 1, 1))
        conv3_3_3x3     = self.conv3_3_3x3(conv3_3_3x3_pad)
        conv3_3_3x3_bn  = self.conv3_3_3x3_bn(conv3_3_3x3)
        activation_18   = F.relu(conv3_3_3x3_bn)
        conv3_3_1x1_increase = self.conv3_3_1x1_increase(activation_18)
        conv3_3_1x1_increase_bn = self.conv3_3_1x1_increase_bn(conv3_3_1x1_increase)
        add_6           = conv3_3_1x1_increase_bn + activation_16
        activation_19   = F.relu(add_6)
        conv3_4_1x1_reduce = self.conv3_4_1x1_reduce(activation_19)
        conv3_4_1x1_reduce_bn = self.conv3_4_1x1_reduce_bn(conv3_4_1x1_reduce)
        activation_20   = F.relu(conv3_4_1x1_reduce_bn)
        conv3_4_3x3_pad = F.pad(activation_20, (1, 1, 1, 1))
        conv3_4_3x3     = self.conv3_4_3x3(conv3_4_3x3_pad)
        conv3_4_3x3_bn  = self.conv3_4_3x3_bn(conv3_4_3x3)
        activation_21   = F.relu(conv3_4_3x3_bn)
        conv3_4_1x1_increase = self.conv3_4_1x1_increase(activation_21)
        conv3_4_1x1_increase_bn = self.conv3_4_1x1_increase_bn(conv3_4_1x1_increase)
        add_7           = conv3_4_1x1_increase_bn + activation_19
        activation_22   = F.relu(add_7)
        conv4_1_1x1_reduce = self.conv4_1_1x1_reduce(activation_22)
        conv4_1_1x1_proj = self.conv4_1_1x1_proj(activation_22)
        conv4_1_1x1_reduce_bn = self.conv4_1_1x1_reduce_bn(conv4_1_1x1_reduce)
        conv4_1_1x1_proj_bn = self.conv4_1_1x1_proj_bn(conv4_1_1x1_proj)
        activation_23   = F.relu(conv4_1_1x1_reduce_bn)
        conv4_1_3x3_pad = F.pad(activation_23, (1, 1, 1, 1))
        conv4_1_3x3     = self.conv4_1_3x3(conv4_1_3x3_pad)
        conv4_1_3x3_bn  = self.conv4_1_3x3_bn(conv4_1_3x3)
        activation_24   = F.relu(conv4_1_3x3_bn)
        conv4_1_1x1_increase = self.conv4_1_1x1_increase(activation_24)
        conv4_1_1x1_increase_bn = self.conv4_1_1x1_increase_bn(conv4_1_1x1_increase)
        add_8           = conv4_1_1x1_increase_bn + conv4_1_1x1_proj_bn
        activation_25   = F.relu(add_8)
        conv4_2_1x1_reduce = self.conv4_2_1x1_reduce(activation_25)
        conv4_2_1x1_reduce_bn = self.conv4_2_1x1_reduce_bn(conv4_2_1x1_reduce)
        activation_26   = F.relu(conv4_2_1x1_reduce_bn)
        conv4_2_3x3_pad = F.pad(activation_26, (1, 1, 1, 1))
        conv4_2_3x3     = self.conv4_2_3x3(conv4_2_3x3_pad)
        conv4_2_3x3_bn  = self.conv4_2_3x3_bn(conv4_2_3x3)
        activation_27   = F.relu(conv4_2_3x3_bn)
        conv4_2_1x1_increase = self.conv4_2_1x1_increase(activation_27)
        conv4_2_1x1_increase_bn = self.conv4_2_1x1_increase_bn(conv4_2_1x1_increase)
        add_9           = conv4_2_1x1_increase_bn + activation_25
        activation_28   = F.relu(add_9)
        conv4_3_1x1_reduce = self.conv4_3_1x1_reduce(activation_28)
        conv4_3_1x1_reduce_bn = self.conv4_3_1x1_reduce_bn(conv4_3_1x1_reduce)
        activation_29   = F.relu(conv4_3_1x1_reduce_bn)
        conv4_3_3x3_pad = F.pad(activation_29, (1, 1, 1, 1))
        conv4_3_3x3     = self.conv4_3_3x3(conv4_3_3x3_pad)
        conv4_3_3x3_bn  = self.conv4_3_3x3_bn(conv4_3_3x3)
        activation_30   = F.relu(conv4_3_3x3_bn)
        conv4_3_1x1_increase = self.conv4_3_1x1_increase(activation_30)
        conv4_3_1x1_increase_bn = self.conv4_3_1x1_increase_bn(conv4_3_1x1_increase)
        add_10          = conv4_3_1x1_increase_bn + activation_28
        activation_31   = F.relu(add_10)
        conv4_4_1x1_reduce = self.conv4_4_1x1_reduce(activation_31)
        conv4_4_1x1_reduce_bn = self.conv4_4_1x1_reduce_bn(conv4_4_1x1_reduce)
        activation_32   = F.relu(conv4_4_1x1_reduce_bn)
        conv4_4_3x3_pad = F.pad(activation_32, (1, 1, 1, 1))
        conv4_4_3x3     = self.conv4_4_3x3(conv4_4_3x3_pad)
        conv4_4_3x3_bn  = self.conv4_4_3x3_bn(conv4_4_3x3)
        activation_33   = F.relu(conv4_4_3x3_bn)
        conv4_4_1x1_increase = self.conv4_4_1x1_increase(activation_33)
        conv4_4_1x1_increase_bn = self.conv4_4_1x1_increase_bn(conv4_4_1x1_increase)
        add_11          = conv4_4_1x1_increase_bn + activation_31
        activation_34   = F.relu(add_11)
        conv4_5_1x1_reduce = self.conv4_5_1x1_reduce(activation_34)
        conv4_5_1x1_reduce_bn = self.conv4_5_1x1_reduce_bn(conv4_5_1x1_reduce)
        activation_35   = F.relu(conv4_5_1x1_reduce_bn)
        conv4_5_3x3_pad = F.pad(activation_35, (1, 1, 1, 1))
        conv4_5_3x3     = self.conv4_5_3x3(conv4_5_3x3_pad)
        conv4_5_3x3_bn  = self.conv4_5_3x3_bn(conv4_5_3x3)
        activation_36   = F.relu(conv4_5_3x3_bn)
        conv4_5_1x1_increase = self.conv4_5_1x1_increase(activation_36)
        conv4_5_1x1_increase_bn = self.conv4_5_1x1_increase_bn(conv4_5_1x1_increase)
        add_12          = conv4_5_1x1_increase_bn + activation_34
        activation_37   = F.relu(add_12)
        conv4_6_1x1_reduce = self.conv4_6_1x1_reduce(activation_37)
        conv4_6_1x1_reduce_bn = self.conv4_6_1x1_reduce_bn(conv4_6_1x1_reduce)
        activation_38   = F.relu(conv4_6_1x1_reduce_bn)
        conv4_6_3x3_pad = F.pad(activation_38, (1, 1, 1, 1))
        conv4_6_3x3     = self.conv4_6_3x3(conv4_6_3x3_pad)
        conv4_6_3x3_bn  = self.conv4_6_3x3_bn(conv4_6_3x3)
        activation_39   = F.relu(conv4_6_3x3_bn)
        conv4_6_1x1_increase = self.conv4_6_1x1_increase(activation_39)
        conv4_6_1x1_increase_bn = self.conv4_6_1x1_increase_bn(conv4_6_1x1_increase)
        add_13          = conv4_6_1x1_increase_bn + activation_37
        activation_40   = F.relu(add_13)
        conv5_1_1x1_reduce = self.conv5_1_1x1_reduce(activation_40)
        conv5_1_1x1_proj = self.conv5_1_1x1_proj(activation_40)
        conv5_1_1x1_reduce_bn = self.conv5_1_1x1_reduce_bn(conv5_1_1x1_reduce)
        conv5_1_1x1_proj_bn = self.conv5_1_1x1_proj_bn(conv5_1_1x1_proj)
        activation_41   = F.relu(conv5_1_1x1_reduce_bn)
        conv5_1_3x3_pad = F.pad(activation_41, (1, 1, 1, 1))
        conv5_1_3x3     = self.conv5_1_3x3(conv5_1_3x3_pad)
        conv5_1_3x3_bn  = self.conv5_1_3x3_bn(conv5_1_3x3)
        activation_42   = F.relu(conv5_1_3x3_bn)
        conv5_1_1x1_increase = self.conv5_1_1x1_increase(activation_42)
        conv5_1_1x1_increase_bn = self.conv5_1_1x1_increase_bn(conv5_1_1x1_increase)
        add_14          = conv5_1_1x1_increase_bn + conv5_1_1x1_proj_bn
        activation_43   = F.relu(add_14)
        conv5_2_1x1_reduce = self.conv5_2_1x1_reduce(activation_43)
        conv5_2_1x1_reduce_bn = self.conv5_2_1x1_reduce_bn(conv5_2_1x1_reduce)
        activation_44   = F.relu(conv5_2_1x1_reduce_bn)
        conv5_2_3x3_pad = F.pad(activation_44, (1, 1, 1, 1))
        conv5_2_3x3     = self.conv5_2_3x3(conv5_2_3x3_pad)
        conv5_2_3x3_bn  = self.conv5_2_3x3_bn(conv5_2_3x3)
        activation_45   = F.relu(conv5_2_3x3_bn)
        conv5_2_1x1_increase = self.conv5_2_1x1_increase(activation_45)
        conv5_2_1x1_increase_bn = self.conv5_2_1x1_increase_bn(conv5_2_1x1_increase)
        add_15          = conv5_2_1x1_increase_bn + activation_43
        activation_46   = F.relu(add_15)
        conv5_3_1x1_reduce = self.conv5_3_1x1_reduce(activation_46)
        conv5_3_1x1_reduce_bn = self.conv5_3_1x1_reduce_bn(conv5_3_1x1_reduce)
        activation_47   = F.relu(conv5_3_1x1_reduce_bn)
        conv5_3_3x3_pad = F.pad(activation_47, (1, 1, 1, 1))
        conv5_3_3x3     = self.conv5_3_3x3(conv5_3_3x3_pad)
        conv5_3_3x3_bn  = self.conv5_3_3x3_bn(conv5_3_3x3)
        activation_48   = F.relu(conv5_3_3x3_bn)
        conv5_3_1x1_increase = self.conv5_3_1x1_increase(activation_48)
        conv5_3_1x1_increase_bn = self.conv5_3_1x1_increase_bn(conv5_3_1x1_increase)
        add_16          = conv5_3_1x1_increase_bn + activation_46
        activation_49   = F.relu(add_16)
        avg_pool        = F.avg_pool2d(activation_49, kernel_size=(7, 7), stride=(7, 7), padding=(0,), ceil_mode=False, count_include_pad=False)
        global_average_pooling2d_1 = F.avg_pool2d(input = avg_pool, kernel_size = avg_pool.size()[2:])
        global_average_pooling2d_1_flatten = global_average_pooling2d_1.view(global_average_pooling2d_1.size(0), -1)
        return global_average_pooling2d_1_flatten


    @staticmethod
    def __batch_normalization(dim, name, **kwargs):
        if   dim == 0 or dim == 1:  layer = nn.BatchNorm1d(**kwargs)
        elif dim == 2:  layer = nn.BatchNorm2d(**kwargs)
        elif dim == 3:  layer = nn.BatchNorm3d(**kwargs)
        else:           raise NotImplementedError()

        if _weights_dict is not None and 'scale' in _weights_dict[name]:
            layer.state_dict()['weight'].copy_(torch.from_numpy(_weights_dict[name]['scale']))
        else:
            layer.weight.data.fill_(1)

        if _weights_dict is not None and 'bias' in _weights_dict[name]:
            layer.state_dict()['bias'].copy_(torch.from_numpy(_weights_dict[name]['bias']))
        else:
            layer.bias.data.fill_(0)

        if _weights_dict is not None:
            layer.state_dict()['running_mean'].copy_(torch.from_numpy(_weights_dict[name]['mean']))
            layer.state_dict()['running_var'].copy_(torch.from_numpy(_weights_dict[name]['var']))
        return layer

    @staticmethod
    def __conv(dim, name, **kwargs):
        if   dim == 1:  layer = nn.Conv1d(**kwargs)
        elif dim == 2:  layer = nn.Conv2d(**kwargs)
        elif dim == 3:  layer = nn.Conv3d(**kwargs)
        else:           raise NotImplementedError()

        if _weights_dict is not None:
            layer.state_dict()['weight'].copy_(torch.from_numpy(_weights_dict[name]['weights']))
            if 'bias' in _weights_dict[name]:
                layer.state_dict()['bias'].copy_(torch.from_numpy(_weights_dict[name]['bias']))

        return layer

